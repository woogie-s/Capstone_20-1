{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Hannanum\n",
    "#from konlpy.tag import Mecab\n",
    "import MeCab\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import sys\n",
    "# 형태소 분석을 위한 객체 생성.\n",
    "kkma = Kkma()\n",
    "komoran = Komoran()\n",
    "hannanum = Hannanum()\n",
    "mecab = MeCab.Tagger()\n",
    "#mecab = Mecab()\n",
    "okt = Okt()\n",
    "from openpyxl import Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('C:/capstone/review/238343_trainReviews.csv',usecols = ['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'광고가너무많이나오고 너무짜증나요그리고만든분들깨는제송하지만 그레도 광구는너무많이나와여 삭제를할까 말까 생각하는데 시간이 2시30분걸렸어요 (제가 많이나오는걸 싫어해서)진짜 만들분깨는 죄송해요 뭐 좋아하는 사람들도 있겠지만 전실력도 그러코나이가 이제 두자리수라서 좀이해좀해주세요 (진짜 죄송 합니다.)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_content = reviews['content'][sample_index]\n",
    "sample_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mecab_nouns(text):\n",
    "    nouns = []\n",
    "    \n",
    "   \n",
    "    pattern = re.compile(\".*\\t[A-Z]+\") \n",
    "    \n",
    "    temp = [tuple(pattern.match(token).group(0).split(\"\\t\")) for token in mecab.parse(text).splitlines()[:-1]]\n",
    "        \n",
    "    for token in temp:\n",
    "        if token[1] == \"NNG\" or token[1] == \"NNP\" or token[1] == \"NNB\" or token[1] == \"NNBC\" or token[1] == \"NP\" or token[1] == \"NR\":\n",
    "            nouns.append(token[0])\n",
    "        \n",
    "    return nouns\n",
    "\n",
    "def mecab_morphs(text):\n",
    "    morphs = []\n",
    "    \n",
    "    pattern = re.compile(\".*\\t[A-Z]+\") \n",
    "    \n",
    "    temp = [tuple(pattern.match(token).group(0).split(\"\\t\")) for token in mecab.parse(text).splitlines()[:-1]]\n",
    "        \n",
    "    for token in temp:\n",
    "        morphs.append(token[0])\n",
    "    \n",
    "    return morphs\n",
    "\n",
    "def mecab_pos(text):\n",
    "    pos = []\n",
    "    pattern = re.compile(\".*\\t[A-Z]+\") \n",
    "    \n",
    "    pos = [tuple(pattern.match(token).group(0).split(\"\\t\")) for token in mecab.parse(text).splitlines()[:-1]]\n",
    "        \n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(doc):\n",
    "    doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", str(doc))\n",
    "    return doc\n",
    "\n",
    "def define_stopwords(path):\n",
    "    \n",
    "    SW = set()\n",
    "    \n",
    "    with open(path) as f:\n",
    "        for word in f:\n",
    "            SW.add(word)\n",
    "            \n",
    "    return SW\n",
    "\n",
    "def text_tokenizing(doc):\n",
    "    \n",
    "    \n",
    "    return [word for word in mecab_morphs(doc) if word not in SW and len(word) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 :  광고가너무많이나오고 너무짜증나요그리고만든분들깨는제송하지만 그레도 광구는너무많이나와여 삭제를할까 말까 생각하는데 시간이 시분걸렸어요 제가 많이나오는걸 싫어해서진짜 만들분깨는 죄송해요 뭐 좋아하는 사람들도 있겠지만 전실력도 그러코나이가 이제 두자리수라서 좀이해좀해주세요 진짜 죄송 합니다\n"
     ]
    }
   ],
   "source": [
    "rp ='C:/capstone/stopwords-ko.txt'\n",
    "SW = define_stopwords(rp)\n",
    "\n",
    "cleaned_text = text_cleaning(sample_content)\n",
    "print(\"전처리 : \", cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'광고가너무많이나오고 너무짜증나요그리고만든분들깨는제송하지만 그레도 광구는너무많이나와여 삭제를할까 말까 생각하는데 시간이 2시30분걸렸어요 (제가 많이나오는걸 싫어해서)진짜 만들분깨는 죄송해요 뭐 좋아하는 사람들도 있겠지만 전실력도 그러코나이가 이제 두자리수라서 좀이해좀해주세요 (진짜 죄송 합니다.)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_content[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = reviews['content'].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sentences.apply(text_tokenizing)\n",
    "tokens[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(tokens, size=100, window = 2, min_count=30, workers=4, iter=100, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Word2Vec.model'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3839"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12035152  0.47353056  0.03151262  0.24145196 -0.0499578   0.00555436\n",
      " -0.12753826 -0.01452407  0.16299035 -0.3883842   0.03437639 -0.01153509\n",
      "  0.33819607  0.02519484  0.14524224  0.09556047 -0.07403176  0.22147293\n",
      "  0.32260087 -0.46824315  0.11855803 -0.07381021  0.0906134  -0.40343058\n",
      " -0.16246295  0.15139224  0.18310682 -0.11141011  0.3029679  -0.09521253\n",
      " -0.0148033  -0.05234611  0.31863967  0.6331645   0.034486    0.46990848\n",
      "  0.14016902  0.37312153 -0.5403111   0.31692982 -0.04429199 -0.07908753\n",
      "  0.10308129 -0.15308769 -0.03420846 -0.044536   -0.15826918  0.18630761\n",
      "  0.43432605  0.13253006  0.3527908   0.36492482 -0.54981095  0.0435458\n",
      " -0.09606533 -0.0414775  -0.29363754  0.2088242   0.05020631  0.08258336\n",
      "  0.2775085   0.0591213   0.2072659   0.36671042 -0.14305277  0.09494033\n",
      " -0.12703456  0.00188303 -0.02407075  0.12104819  0.02160708  0.21285458\n",
      "  0.07130171  0.01532427  0.28976825  0.06779323  0.2112447  -0.38877007\n",
      " -0.24840924 -0.03128318  0.49952248 -0.20197207 -0.01595896  0.25077474\n",
      "  0.05422786 -0.23065773 -0.01322305 -0.22751597  0.04317298  0.2436318\n",
      "  0.0071767  -0.360088    0.20913473 -0.28707758  0.08006299  0.07152687\n",
      "  0.1116806   0.3944972   0.04428894  0.03468855]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.get_vector('광고'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('릭터', 0.7168731689453125),\n",
       " ('캐릭', 0.6835006475448608),\n",
       " ('개성', 0.6018631458282471),\n",
       " ('스킨', 0.5742120742797852),\n",
       " ('영웅', 0.5298223495483398),\n",
       " ('디자인', 0.5136014223098755),\n",
       " ('매력', 0.5050417184829712),\n",
       " ('마음', 0.4868336319923401),\n",
       " ('무엇', 0.4865906238555908),\n",
       " ('케릭', 0.4701857566833496)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('캐릭터')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(vocab)[:]\n",
    "vocab_length = len(vocab_list)\n",
    "word_metrix = np.zeros((vocab_length,100))\n",
    "\n",
    "#word_metrix = [model.wv.get_vector(v) for v in model.wv.vocab.keys()] \n",
    "#단어행렬\n",
    "for i in range(vocab_length):\n",
    "        word_metrix[i] = model.wv.get_vector(vocab_list[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#거리 행렬과 가중치 행렬(정규분포 적용, 정규분포에 가까워지도록 하는 분산 값 확인 필요) \n",
    "distance_matrix = euclidean_distances(word_metrix, word_metrix)\n",
    "weight_matrix = np.exp(-(distance_matrix ** 2) / (2 * np.var(distance_matrix)))\n",
    "\n",
    "#print(distance_matrix)\n",
    "#print(weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = weight_matrix[640]#결제\n",
    "a2 = weight_matrix[948] #계정\n",
    "a3 = weight_matrix[301] #서버\n",
    "a4 = weight_matrix[668] #구성\n",
    "a5 = weight_matrix[772] #연출\n",
    "a6 = weight_matrix[258] #캐릭터\n",
    "a7 = weight_matrix[78] #시스템\n",
    "\n",
    "#카테고리 분류한 weight_matrix\n",
    "W_M = np.array([a1,a2,a3,a4,a5,a6,a7])\n",
    "#print(W_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TDM 설계\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer \n",
    "vectorizer = CountVectorizer(vocabulary= vocab_list, binary= bool)\n",
    "#print(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = vectorizer.fit_transform(sentences)\n",
    "#print(X2.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#내적을 위해 트랜스폼\n",
    "X3 = X2.toarray().T\n",
    "#print(X3)\n",
    "#print(W_M.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#내적\n",
    "function_score = np.dot(W_M , X3[ : ,28:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.42724203e-03]\n",
      " [6.53944430e-03]\n",
      " [1.63311944e-02]\n",
      " [7.08792761e-03]\n",
      " [5.68226480e-05]\n",
      " [1.01016596e-01]\n",
      " [2.32844475e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(function_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#그 리뷰가 어느 기능에 속하는지\n",
    "for i in range(7):\n",
    "    if function_score[i] == max(function_score):\n",
    "        last_category = i\n",
    "\n",
    "last_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
